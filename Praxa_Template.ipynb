{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharmaprateek/scripts/blob/master/Praxa_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz1fotPlr1m3"
      },
      "source": [
        "#Lesson 3: Vector Database\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-Nc1lD3tYag"
      },
      "source": [
        "##Getting Started\n",
        "If you're new to Google Colab, download and review the [Getting Started with Colab](https://uploads.smart.ly/assets/49f329a834468c6f6e9010cbf337a2753b22d35c245e49fc00d4b89e4ceb10fa/original/49f329a834468c6f6e9010cbf337a2753b22d35c245e49fc00d4b89e4ceb10fa.pdf) guide.\n",
        "\n",
        "Your code and data will run in the `/content` directory. Create a subdirectory in `/content` called `context_data` and upload the [context documents for the course](https://uploads.smart.ly/assets/9af2030979d9b37119354aa47b0ee7e7746e124406400f75a1588f40379b43a1/original/9af2030979d9b37119354aa47b0ee7e7746e124406400f75a1588f40379b43a1.zip) into `context_data`.\n",
        "\n",
        "You'll also need an API key from Hugging Face. Visit their [signup page](https://huggingface.co/join), enter your email and a password, then complete your profile. Once you have an account and are signed in, go to [Settings | Access Tokens](https://huggingface.co/settings/tokens) and select \"New token.\" Write tokens allow you to post to Hugging Face, which you won't be doing here, so you only need a read-type token.\n",
        "\n",
        "Once you have your token, enter it below and run the code in the cell by clicking the play button on its left. Note that all commands at the shell prompt, such as `pip` below, should be preceded with a bang `!`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f06PqYMTv61t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = \"your-API-token\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYxAfzSNzZSt"
      },
      "source": [
        "LangChain touches all aspects of this app, so let's go ahead and install it now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tokWSgM-zvDu"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.1.13 langchain-community==0.0.29 langchain-core==0.1.40 pypdf==4.1.0 langchain-text-splitters==0.0.1 sentence_transformers==2.6.1 langchain-chroma==0.1.0 huggingface_hub==0.23.5 transformers==4.38.2 streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP7ReL0twnJ9"
      },
      "source": [
        "##Loading Context Documents\n",
        "The first step in building the vector database is to load the context documents. Load them into a variable named `context_data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KY4CoAHVz_h3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjgyps9f4uDg"
      },
      "source": [
        "Now let's verify that the documents loaded by printing the content of each page. Scroll to the end of a line to see what metadata the document loader includes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFtvWOhJ4y-w"
      },
      "outputs": [],
      "source": [
        "for page in context_data:\n",
        "  print(page)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1RT2MMk6Jbj"
      },
      "source": [
        "##Chunking\n",
        "Now it's time to split the documents into chunks that will work with the LLM's context window. Store them in a variable named `chunks`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRUcG95A8jqB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofngPyLR_EXV"
      },
      "source": [
        "Verify it worked by exploring how the documents were chunked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx_t2f8U-k9d"
      },
      "outputs": [],
      "source": [
        "print(f\"Total Document Chunks: {len(chunks)}\")\n",
        "print(\"-----\")\n",
        "print(\"Length of each chunk:\")\n",
        "\n",
        "for num, chunk in enumerate(chunks):\n",
        "  print(f\"Chunk {num} (from page {chunk.metadata['page'] + 1}): {len(chunk.page_content)} characters\")\n",
        "\n",
        "print(\"-----\")\n",
        "print(\"Chunk 0\")\n",
        "print(chunks[0].metadata)\n",
        "print(chunks[0].page_content)\n",
        "print(\"-----\")\n",
        "print(\"Chunk 1\")\n",
        "print(chunks[1].metadata)\n",
        "print(chunks[1].page_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnR0lbf-CSm1"
      },
      "source": [
        "##Embedding\n",
        "\n",
        "Now it's time to set up the embedding function. Assign it to a variable named `embedding_function`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln3cVD-XCWIg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYCpOZU9DvI0"
      },
      "source": [
        "Make sure your model works by finding the embedding for a test sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RaQ_S0eDzix"
      },
      "outputs": [],
      "source": [
        "embedding = embedding_function.embed_query(\"This is a test sentence.\")\n",
        "print(f\"Embedding length: {len(embedding)}\")\n",
        "embedding = embedding_function.embed_query(\"This is a longer test sentence.\")\n",
        "print(f\"Embedding length: {len(embedding)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGeqzZA8IQqv"
      },
      "source": [
        "##Persisting\n",
        "\n",
        "Now it's time for the vector store. Assign it the name `chromadb`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-KT3aALInxP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pmh1FmFLIE3"
      },
      "source": [
        "Now test it by executing a similarity search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uwCgpxrLO1_"
      },
      "outputs": [],
      "source": [
        "retrieved_chunks = chromadb.similarity_search(\"Two people who take a vacation together.\")\n",
        "print(f\"Query retrieved {len(retrieved_chunks)} chunks.\")\n",
        "for chunk in retrieved_chunks:\n",
        "  print(f\"Chunk content: {chunk.page_content}\")\n",
        "  print(f\"Chunk metadata: {chunk.metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-b4gTUZ6yPAP"
      },
      "source": [
        "#Lesson 4: LangChain and Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLsPvc7t6Pqn"
      },
      "source": [
        "###Getting the LLM\n",
        "We use the `HuggingFaceHub` API to instantiate the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvs5-3bE6rCI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ9gZIxR__Oq"
      },
      "source": [
        "Let's invoke the LLM with a few prompts it should be able to handle. Take note of the answers, which are based solely on the model's training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xagZYN4GAQA7"
      },
      "outputs": [],
      "source": [
        "response = llm.invoke(\"List Tawfiq al-Hakim's plays by title as a comma-separated list.\")\n",
        "print(response)\n",
        "response = llm.invoke(\"List Jez Butterworth's plays.\")\n",
        "print(response)\n",
        "response = llm.invoke(\"What Broadway plays have had over 10,000 performances?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEwwSFj5Dr5L"
      },
      "source": [
        "###Setting up a Prompt Template\n",
        "We'll now build a simple prompt template to make our interface with the LLM a bit more generic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2gdIykGEBNQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kXZPOpcEkvH"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW-7mJBaEmdU"
      },
      "outputs": [],
      "source": [
        "print(prompt)\n",
        "response = llm.invoke(prompt.format(playwright=\"Jez Butterworth\"))\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL-a_WYkK_dM"
      },
      "source": [
        "###Output Parsers\n",
        "While we're exploring the Model I/O module let's take a quick look at how the output parser in the Quickstart works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YG3g-nlTLXA3"
      },
      "outputs": [],
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "response = output_parser.parse(llm.invoke(prompt.format(playwright=\"Jez Butterworth\")))\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYJ5TPaqPiD8"
      },
      "source": [
        "## LangChain Expression Language (LCEL)\n",
        "The \"Chain\" in \"LangChain\" refers to the ability to chain several actions into one invocation. This replaces your nested calls to `output_parser()`, `llm.invoke()`, and `prompt.format()`. Try to build a chain for what you have here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj-_rm2dQNr1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3VlXuSfB2_5"
      },
      "source": [
        "#Lesson 5: RAG Using LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrMBqouGB_5U"
      },
      "source": [
        "##Build a Prompt Template\n",
        "We'll start with a prompt template that combines the context and original question and provides instructions to the model on how to use both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6LBTX5NCTo7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4vSm7m7FJVt"
      },
      "source": [
        "To get the context, we'll use a *retriever*. It takes a string as the input query and returns a `list` of `Document` objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEEdbwfMFqCv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvgeUSMPF3gs"
      },
      "source": [
        "Run it to see what it outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TZ0fq9UF6CN"
      },
      "outputs": [],
      "source": [
        "docs = retriever.get_relevant_documents(\"List Jez Butterworth's plays.\")\n",
        "print(f\"Found {len(docs)} documents:\")\n",
        "\n",
        "for doc in docs:\n",
        "  print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgki18WqiXZj"
      },
      "source": [
        "The final form we're going for is `chain.invoke(user_question)`. We'll need the `user_question` for two things in this prompt: the question itself and finding the context from the vector database. Doing multiple things to one input is the job of a `RunnableParallel`. Let's create one that does that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meXqYUTfieuY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlQnER6Cis8Z"
      },
      "source": [
        "Let's see what that looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8gW0YKlivfC"
      },
      "outputs": [],
      "source": [
        "context_and_question.invoke(\"List Jez Butterworth's plays.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr3BOS2tJvch"
      },
      "source": [
        "To use the context docs in a prompt, we're going to need to convert them to a string. We'll use a `RunnablePassthrough` to assign that string to the `context` key the prompt needs. Note that the `question` attribute from `context_docs_and_question` gets passed through."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuTlQxcWJvIh"
      },
      "outputs": [],
      "source": [
        "def convert_context_docs(to_convert):\n",
        "    # Take the page_content attribute of each Document object\n",
        "    # and join them into one string, separated by two newlines.\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in to_convert[\"context_docs\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HfjcNldLqlF"
      },
      "source": [
        "Let's see how all this works with our prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcTaUM1aLt-a"
      },
      "outputs": [],
      "source": [
        "complete_prompt_chain = context_and_question | convert_context | prompt\n",
        "complete_prompt_chain.invoke(\"List Jez Butterworth's plays.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDzUuMVyMLF1"
      },
      "source": [
        "Now we'll build the final chain for our app."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAiLsZ0gMPnT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Tx95TvIMW5l"
      },
      "source": [
        "And run it to see what results we get. Here we should see that \"The Hills of California\" is included in the list of plays, even though it occurred after the training cutoff of the model.\n",
        "\n",
        "If you don't see \"The Hills of California\" at the bottom, try starting a new Colab runtime and running the code in the notebook again. This resets the model and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwaDZ1jZMaBJ"
      },
      "outputs": [],
      "source": [
        "result = chain.invoke(\"List Jez Butterworth's plays.\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiAIHm-WfIbp"
      },
      "source": [
        "Now we'll build a chain that passes the source citations, which were in the metadata field of the `list` of `Document` objects returned from the retriever. We'll use `RunnableParallel` to pass the `list` to the end of the chain while also passing it to a chain that builds the prompt and invokes the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdhhFAO4f-Uo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCAcF-Y3glRJ"
      },
      "source": [
        "Now run it to see what we got. When we asked this question without context, the model told us that \"The Phantom of the Opera\" was the only play that had more than 10,000 performances. When the context data is included, we add \"Chicago,\" \"The Lion King,\" and \"Wicked.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oc8HoXULp88X"
      },
      "outputs": [],
      "source": [
        "result = chain_with_sources.invoke(\"What Broadway plays have had over 10,000 performances?\")\n",
        "print(\"The docs used in this answer:\")\n",
        "print(\"\\n\".join(doc.metadata.__repr__() for doc in result[\"context_docs\"]))\n",
        "print(\"\\nThe answer:\")\n",
        "print(result[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfdS__hkl8g3"
      },
      "source": [
        "#Lesson 6: User Interface\n",
        "While not directly related to LLMs or AI in general, user interfaces are essential to making an app approachable. We'll use Streamlit to build a basic front end for our app.\n",
        "##Getting Started\n",
        "First we need to install an npm package that will allow us to expose the Colab runtime to IP traffic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_HnWFCYnwjD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8cGFC8MndmL"
      },
      "source": [
        "Now we'll create a simple \"Hello World\" app. This illustrates how simple Streamlit is to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMauEgLqoHts"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q8ximhRoPW0"
      },
      "source": [
        "Now we need to run the app and view it in a browser. There are a few steps to this:\n",
        "* Start the Streamlit server using the *app.py* script.\n",
        "* Set up a local tunnel to get a URL that connects to the Colab runtime.\n",
        "* Get the public IP of the Colab runtime to gain access to the localtunnel-created URL.\n",
        "\n",
        "We'll do this all on one command line. The command will display the public IP of the Colab runtime then a link to the Streamlit server. When you click the link you'll be asked for the tunnel password, which is the Colab runtime's public IP.\n",
        "\n",
        "Note that this cell will continue running the server until you manually stop it. No other cells in the notebook can run while this cell is running. Stop the cell by selecting the stop button to its left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1qjxpn_pP_U"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpbw1ZpjqnWV"
      },
      "source": [
        "##Building the Backend\n",
        "Up to this point we've been running the Python instructions in interactive mode in the Colab notebook. For our app to work as a backend, we need to make the code available in a module that the front end can import. Let's go back through our code and copy the needed elements into a single Python file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7inJlGkWrNkh"
      },
      "outputs": [],
      "source": [
        "%%writefile backend.py\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableParallel\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"\"\"\n",
        "You are an assistant providing answers to questions about\n",
        "the theater. In addition to your training data, you are to\n",
        "use the additional context provided below to provide\n",
        "up-to-date information.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\")\n",
        "\n",
        "embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "vectordb = Chroma(persist_directory=\"./chromadb\",\n",
        "                  embedding_function=embedding_function)\n",
        "retriever = vectordb.as_retriever()\n",
        "\n",
        "context_and_question = RunnableParallel(\n",
        "    {\"context_docs\": retriever, \"question\": RunnablePassthrough()}\n",
        ")\n",
        "\n",
        "def convert_context_docs(to_convert):\n",
        "    # Take the page_content attribute of each Document object\n",
        "    # and join them into one string, separated by two newlines.\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in to_convert[\"context_docs\"])\n",
        "\n",
        "convert_context = RunnablePassthrough.assign(context=convert_context_docs)\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    task=\"text-generation\",\n",
        "    model_kwargs={\n",
        "        \"max_new_tokens\": 512,\n",
        "        \"top_k\": 30,\n",
        "        \"temperature\": 0.1,\n",
        "        \"repetition_penalty\": 1.03,\n",
        "    },\n",
        ")\n",
        "\n",
        "answer_chain = convert_context | prompt | llm\n",
        "chain_with_sources = context_and_question.assign(answer=answer_chain)\n",
        "\n",
        "def answer_and_sources(question):\n",
        "    result = chain_with_sources.invoke(question)\n",
        "    response_text = result[\"answer\"]\n",
        "    answer_index = response_text.rfind(\"Answer:\")\n",
        "    answer_text = response_text[answer_index + len(\"Answer:\"):].strip()\n",
        "    sources = \"\\n\\n\".join(f\"{doc.metadata['source']}, page {doc.metadata['page']}\" for doc in result[\"context_docs\"])\n",
        "    return {\"answer\": answer_text,\n",
        "            \"sources\": sources\n",
        "            }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NshhzFpfycmu"
      },
      "source": [
        "Now test the back end manually to make sure it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_XXDxW8ygky"
      },
      "outputs": [],
      "source": [
        "import backend, importlib\n",
        "importlib.reload(backend)\n",
        "print(backend.answer_and_sources(\"List Jez Butterworth's plays.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgBSvp4T21Rf"
      },
      "source": [
        "##Building the Interface\n",
        "Now let's use Streamlit's example chat app to build the interface for Praxa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCUV65PErwP9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu-P0k8g3lpl"
      },
      "source": [
        "Now we run the whole app."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX4RcudXAo4N"
      },
      "outputs": [],
      "source": [
        "!streamlit run praxa.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}